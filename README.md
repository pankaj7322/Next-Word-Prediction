# Next word prediction with TensorFlow

This project demonstrates the creation of a text prediction model using TensorFlow. The model is trained on an example letter (referred to as 'faqs') and utilizes Natural Language Processing (NLP) techniques to generate the next word in a sequence based on previous input.

## Overview

This project shows the implementation of a basic text generation model using LSTM (Long Short-Term Memory) layers and an embedding layer in TensorFlow. The model is designed to take a sequence of words as input and predict the next word in the sequence.

## Prerequisites

Before running this project, ensure you have the following dependencies installed:

- Python 3.x
- TensorFlow 2.x
- NumPy
- Keras

You can install the required dependencies using the following command:

```bash
pip install tensorflow numpy
```

## How to Run

To run the model, follow these steps:

Clone this repository to your local machine.
```bash
git clone https://github.com/pankaj7322/Next-Word-Prediction.git
cd Next-Word-Prediction
```
## Install the dependencies:
```bash
pip install tensorflow numpy
```
## Run the script:
```bash
python ai_python_model.py
```
The model will begin training on the provided text and then generate predictions based on a seed input (e.g., "Hiring for").

The output will be a sequence of words predicted by the model, one word at a time, based on the given input text.

## Model Architecture

**The model uses the following architecture:**

Embedding Layer: Transforms the input tokens into dense vectors of fixed size.
LSTM Layer: Uses Long Short-Term Memory units to learn the sequential dependencies in the text data.
Dense Layer: The output layer, which predicts the next word in the sequence using a softmax activation function.

## Training

**The model is trained on the tokenized version of the input text, where:**

The input sequences are generated by breaking the text into smaller chunks (subsequences).
The output is the next word in the sequence.
The loss function used is categorical cross-entropy, and the optimizer is Adam.

## Example Prediction

The script takes an input string (e.g., "Hiring for") and predicts the next word in the sequence based on the model's training. The prediction will generate new words each time you run the model.

## Future Improvements

Train the model on a larger and more varied dataset to improve predictions.
Implement different architectures like GRU or transformer-based models.
Fine-tune hyperparameters for better performance.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

Feel free to save this as `README.md` in your project directory! It provides an overview of the project, instructions for running the model, and information about its structure.

Let me know if you need any adjustments!
