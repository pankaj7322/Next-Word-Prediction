{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "LemZ3MZhJM6f"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OXfBlTiuvXd1"
      },
      "outputs": [],
      "source": [
        "faqs = \"\"\"\n",
        "  Pankaj Kumar\n",
        "  Shivbag Colony, Ameerpet\n",
        "  Hyderabad, Telangana, 500016\n",
        "  Pankajkumar732298@gmail.com\n",
        "  +917322985823\n",
        "  23-07-2024\n",
        "  Hiring Manager\n",
        "  ACCESS ASSIST\n",
        "  Dear Hiring Manager,\n",
        "  I am writing to express my interest in the AI Python Developer position at ACCESS ASSIST.\n",
        "  With a recent completion of a comprehensive Full Stack Data Science course and a degree in\n",
        "  MCA (AIML) from LNCT University, I am eager to apply my skills and knowledge in Generative\n",
        "  AI through your renowned training program.\n",
        "  The rigorous, hands-on experience offered by your 4-month training program is exactly what I\n",
        "  am looking for to bridge the gap between academic learning and real-world application. During\n",
        "  my Full Stack Data Science course, I gained expertise in Data Science, AI, Machine Learning,\n",
        "  and NLP, which I am excited to further develop through your intensive curriculum.\n",
        "  I am particularly enthusiastic about the opportunity to work with diverse international teams on\n",
        "  impactful global projects. My experience with multiple projects has prepared me to collaborate\n",
        "  effectively in team settings and tackle complex data challenges.\n",
        "  ACCESS ASSIST commitment to continuous learning and innovation aligns perfectly with my\n",
        "  career goals. I am eager to immerse myself in your training program and contribute to\n",
        "  developing AI-driven solutions that address significant global issues.\n",
        "  Thank you for considering my application. I look forward to the opportunity to discuss how my\n",
        "  background, skills, and aspirations align with the goals of ACCESS ASSIST.\n",
        "  Sincerely,\n",
        "  Pankaj Kumar\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "Nj-qSi1yxdWv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "6y-PJhh6xrMi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKJUxB_xtoD",
        "outputId": "71a32da4-5b36-446d-ebf5-ecef69d3dd57"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': 1,\n",
              " 'i': 2,\n",
              " 'and': 3,\n",
              " 'my': 4,\n",
              " 'am': 5,\n",
              " 'in': 6,\n",
              " 'the': 7,\n",
              " 'with': 8,\n",
              " 'access': 9,\n",
              " 'assist': 10,\n",
              " 'ai': 11,\n",
              " 'data': 12,\n",
              " 'your': 13,\n",
              " 'a': 14,\n",
              " 'science': 15,\n",
              " 'training': 16,\n",
              " 'program': 17,\n",
              " 'learning': 18,\n",
              " 'pankaj': 19,\n",
              " 'kumar': 20,\n",
              " 'hiring': 21,\n",
              " 'manager': 22,\n",
              " 'of': 23,\n",
              " 'full': 24,\n",
              " 'stack': 25,\n",
              " 'course': 26,\n",
              " 'eager': 27,\n",
              " 'skills': 28,\n",
              " 'through': 29,\n",
              " 'on': 30,\n",
              " 'experience': 31,\n",
              " 'for': 32,\n",
              " 'application': 33,\n",
              " 'opportunity': 34,\n",
              " 'global': 35,\n",
              " 'projects': 36,\n",
              " 'goals': 37,\n",
              " 'shivbag': 38,\n",
              " 'colony': 39,\n",
              " 'ameerpet': 40,\n",
              " 'hyderabad': 41,\n",
              " 'telangana': 42,\n",
              " '500016': 43,\n",
              " 'pankajkumar732298': 44,\n",
              " 'gmail': 45,\n",
              " 'com': 46,\n",
              " '917322985823': 47,\n",
              " '23': 48,\n",
              " '07': 49,\n",
              " '2024': 50,\n",
              " 'dear': 51,\n",
              " 'writing': 52,\n",
              " 'express': 53,\n",
              " 'interest': 54,\n",
              " 'python': 55,\n",
              " 'developer': 56,\n",
              " 'position': 57,\n",
              " 'at': 58,\n",
              " 'recent': 59,\n",
              " 'completion': 60,\n",
              " 'comprehensive': 61,\n",
              " 'degree': 62,\n",
              " 'mca': 63,\n",
              " 'aiml': 64,\n",
              " 'from': 65,\n",
              " 'lnct': 66,\n",
              " 'university': 67,\n",
              " 'apply': 68,\n",
              " 'knowledge': 69,\n",
              " 'generative': 70,\n",
              " 'renowned': 71,\n",
              " 'rigorous': 72,\n",
              " 'hands': 73,\n",
              " 'offered': 74,\n",
              " 'by': 75,\n",
              " '4': 76,\n",
              " 'month': 77,\n",
              " 'is': 78,\n",
              " 'exactly': 79,\n",
              " 'what': 80,\n",
              " 'looking': 81,\n",
              " 'bridge': 82,\n",
              " 'gap': 83,\n",
              " 'between': 84,\n",
              " 'academic': 85,\n",
              " 'real': 86,\n",
              " 'world': 87,\n",
              " 'during': 88,\n",
              " 'gained': 89,\n",
              " 'expertise': 90,\n",
              " 'machine': 91,\n",
              " 'nlp': 92,\n",
              " 'which': 93,\n",
              " 'excited': 94,\n",
              " 'further': 95,\n",
              " 'develop': 96,\n",
              " 'intensive': 97,\n",
              " 'curriculum': 98,\n",
              " 'particularly': 99,\n",
              " 'enthusiastic': 100,\n",
              " 'about': 101,\n",
              " 'work': 102,\n",
              " 'diverse': 103,\n",
              " 'international': 104,\n",
              " 'teams': 105,\n",
              " 'impactful': 106,\n",
              " 'multiple': 107,\n",
              " 'has': 108,\n",
              " 'prepared': 109,\n",
              " 'me': 110,\n",
              " 'collaborate': 111,\n",
              " 'effectively': 112,\n",
              " 'team': 113,\n",
              " 'settings': 114,\n",
              " 'tackle': 115,\n",
              " 'complex': 116,\n",
              " 'challenges': 117,\n",
              " 'commitment': 118,\n",
              " 'continuous': 119,\n",
              " 'innovation': 120,\n",
              " 'aligns': 121,\n",
              " 'perfectly': 122,\n",
              " 'career': 123,\n",
              " 'immerse': 124,\n",
              " 'myself': 125,\n",
              " 'contribute': 126,\n",
              " 'developing': 127,\n",
              " 'driven': 128,\n",
              " 'solutions': 129,\n",
              " 'that': 130,\n",
              " 'address': 131,\n",
              " 'significant': 132,\n",
              " 'issues': 133,\n",
              " 'thank': 134,\n",
              " 'you': 135,\n",
              " 'considering': 136,\n",
              " 'look': 137,\n",
              " 'forward': 138,\n",
              " 'discuss': 139,\n",
              " 'how': 140,\n",
              " 'background': 141,\n",
              " 'aspirations': 142,\n",
              " 'align': 143,\n",
              " 'sincerely': 144}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in faqs.split('\\n'):\n",
        "   print(([sentence]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YKnQ4X5zau5",
        "outputId": "75114747-6635-42cc-c172-f730df78b88a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['']\n",
            "['  Pankaj Kumar']\n",
            "['  Shivbag Colony, Ameerpet']\n",
            "['  Hyderabad, Telangana, 500016']\n",
            "['  Pankajkumar732298@gmail.com']\n",
            "['  +917322985823']\n",
            "['  23-07-2024']\n",
            "['  Hiring Manager']\n",
            "['  ACCESS ASSIST']\n",
            "['  Dear Hiring Manager,']\n",
            "['  I am writing to express my interest in the AI Python Developer position at ACCESS ASSIST.']\n",
            "['  With a recent completion of a comprehensive Full Stack Data Science course and a degree in']\n",
            "['  MCA (AIML) from LNCT University, I am eager to apply my skills and knowledge in Generative']\n",
            "['  AI through your renowned training program.']\n",
            "['  The rigorous, hands-on experience offered by your 4-month training program is exactly what I']\n",
            "['  am looking for to bridge the gap between academic learning and real-world application. During']\n",
            "['  my Full Stack Data Science course, I gained expertise in Data Science, AI, Machine Learning,']\n",
            "['  and NLP, which I am excited to further develop through your intensive curriculum.']\n",
            "['  I am particularly enthusiastic about the opportunity to work with diverse international teams on']\n",
            "['  impactful global projects. My experience with multiple projects has prepared me to collaborate']\n",
            "['  effectively in team settings and tackle complex data challenges.']\n",
            "['  ACCESS ASSIST commitment to continuous learning and innovation aligns perfectly with my']\n",
            "['  career goals. I am eager to immerse myself in your training program and contribute to']\n",
            "['  developing AI-driven solutions that address significant global issues.']\n",
            "['  Thank you for considering my application. I look forward to the opportunity to discuss how my']\n",
            "['  background, skills, and aspirations align with the goals of ACCESS ASSIST.']\n",
            "['  Sincerely,']\n",
            "['  Pankaj Kumar']\n",
            "['']\n",
            "['']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in faqs.split('\\n'):\n",
        "   tokenizer.texts_to_sequences([sentence])[0]"
      ],
      "metadata": {
        "id": "RoR94Qp5x3oy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in faqs.split('\\n'):\n",
        "   tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "   for i in range(1, len(tokenized_sentence)):\n",
        "      input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "b6dY9RwUySIt"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a4DyYPOz2e4",
        "outputId": "19d60387-3809-45c8-8cc4-1e96994cf785"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[19, 20],\n",
              " [38, 39],\n",
              " [38, 39, 40],\n",
              " [41, 42],\n",
              " [41, 42, 43],\n",
              " [44, 45],\n",
              " [44, 45, 46],\n",
              " [48, 49],\n",
              " [48, 49, 50],\n",
              " [21, 22],\n",
              " [9, 10],\n",
              " [51, 21],\n",
              " [51, 21, 22],\n",
              " [2, 5],\n",
              " [2, 5, 52],\n",
              " [2, 5, 52, 1],\n",
              " [2, 5, 52, 1, 53],\n",
              " [2, 5, 52, 1, 53, 4],\n",
              " [2, 5, 52, 1, 53, 4, 54],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7, 11],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7, 11, 55],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7, 11, 55, 56],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7, 11, 55, 56, 57],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7, 11, 55, 56, 57, 58],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7, 11, 55, 56, 57, 58, 9],\n",
              " [2, 5, 52, 1, 53, 4, 54, 6, 7, 11, 55, 56, 57, 58, 9, 10],\n",
              " [8, 14],\n",
              " [8, 14, 59],\n",
              " [8, 14, 59, 60],\n",
              " [8, 14, 59, 60, 23],\n",
              " [8, 14, 59, 60, 23, 14],\n",
              " [8, 14, 59, 60, 23, 14, 61],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25, 12],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25, 12, 15],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25, 12, 15, 26],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25, 12, 15, 26, 3],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25, 12, 15, 26, 3, 14],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25, 12, 15, 26, 3, 14, 62],\n",
              " [8, 14, 59, 60, 23, 14, 61, 24, 25, 12, 15, 26, 3, 14, 62, 6],\n",
              " [63, 64],\n",
              " [63, 64, 65],\n",
              " [63, 64, 65, 66],\n",
              " [63, 64, 65, 66, 67],\n",
              " [63, 64, 65, 66, 67, 2],\n",
              " [63, 64, 65, 66, 67, 2, 5],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1, 68],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1, 68, 4],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1, 68, 4, 28],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1, 68, 4, 28, 3],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1, 68, 4, 28, 3, 69],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1, 68, 4, 28, 3, 69, 6],\n",
              " [63, 64, 65, 66, 67, 2, 5, 27, 1, 68, 4, 28, 3, 69, 6, 70],\n",
              " [11, 29],\n",
              " [11, 29, 13],\n",
              " [11, 29, 13, 71],\n",
              " [11, 29, 13, 71, 16],\n",
              " [11, 29, 13, 71, 16, 17],\n",
              " [7, 72],\n",
              " [7, 72, 73],\n",
              " [7, 72, 73, 30],\n",
              " [7, 72, 73, 30, 31],\n",
              " [7, 72, 73, 30, 31, 74],\n",
              " [7, 72, 73, 30, 31, 74, 75],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76, 77],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76, 77, 16],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76, 77, 16, 17],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76, 77, 16, 17, 78],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76, 77, 16, 17, 78, 79],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76, 77, 16, 17, 78, 79, 80],\n",
              " [7, 72, 73, 30, 31, 74, 75, 13, 76, 77, 16, 17, 78, 79, 80, 2],\n",
              " [5, 81],\n",
              " [5, 81, 32],\n",
              " [5, 81, 32, 1],\n",
              " [5, 81, 32, 1, 82],\n",
              " [5, 81, 32, 1, 82, 7],\n",
              " [5, 81, 32, 1, 82, 7, 83],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84, 85],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84, 85, 18],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84, 85, 18, 3],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84, 85, 18, 3, 86],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84, 85, 18, 3, 86, 87],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84, 85, 18, 3, 86, 87, 33],\n",
              " [5, 81, 32, 1, 82, 7, 83, 84, 85, 18, 3, 86, 87, 33, 88],\n",
              " [4, 24],\n",
              " [4, 24, 25],\n",
              " [4, 24, 25, 12],\n",
              " [4, 24, 25, 12, 15],\n",
              " [4, 24, 25, 12, 15, 26],\n",
              " [4, 24, 25, 12, 15, 26, 2],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89, 90],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89, 90, 6],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89, 90, 6, 12],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89, 90, 6, 12, 15],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89, 90, 6, 12, 15, 11],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89, 90, 6, 12, 15, 11, 91],\n",
              " [4, 24, 25, 12, 15, 26, 2, 89, 90, 6, 12, 15, 11, 91, 18],\n",
              " [3, 92],\n",
              " [3, 92, 93],\n",
              " [3, 92, 93, 2],\n",
              " [3, 92, 93, 2, 5],\n",
              " [3, 92, 93, 2, 5, 94],\n",
              " [3, 92, 93, 2, 5, 94, 1],\n",
              " [3, 92, 93, 2, 5, 94, 1, 95],\n",
              " [3, 92, 93, 2, 5, 94, 1, 95, 96],\n",
              " [3, 92, 93, 2, 5, 94, 1, 95, 96, 29],\n",
              " [3, 92, 93, 2, 5, 94, 1, 95, 96, 29, 13],\n",
              " [3, 92, 93, 2, 5, 94, 1, 95, 96, 29, 13, 97],\n",
              " [3, 92, 93, 2, 5, 94, 1, 95, 96, 29, 13, 97, 98],\n",
              " [2, 5],\n",
              " [2, 5, 99],\n",
              " [2, 5, 99, 100],\n",
              " [2, 5, 99, 100, 101],\n",
              " [2, 5, 99, 100, 101, 7],\n",
              " [2, 5, 99, 100, 101, 7, 34],\n",
              " [2, 5, 99, 100, 101, 7, 34, 1],\n",
              " [2, 5, 99, 100, 101, 7, 34, 1, 102],\n",
              " [2, 5, 99, 100, 101, 7, 34, 1, 102, 8],\n",
              " [2, 5, 99, 100, 101, 7, 34, 1, 102, 8, 103],\n",
              " [2, 5, 99, 100, 101, 7, 34, 1, 102, 8, 103, 104],\n",
              " [2, 5, 99, 100, 101, 7, 34, 1, 102, 8, 103, 104, 105],\n",
              " [2, 5, 99, 100, 101, 7, 34, 1, 102, 8, 103, 104, 105, 30],\n",
              " [106, 35],\n",
              " [106, 35, 36],\n",
              " [106, 35, 36, 4],\n",
              " [106, 35, 36, 4, 31],\n",
              " [106, 35, 36, 4, 31, 8],\n",
              " [106, 35, 36, 4, 31, 8, 107],\n",
              " [106, 35, 36, 4, 31, 8, 107, 36],\n",
              " [106, 35, 36, 4, 31, 8, 107, 36, 108],\n",
              " [106, 35, 36, 4, 31, 8, 107, 36, 108, 109],\n",
              " [106, 35, 36, 4, 31, 8, 107, 36, 108, 109, 110],\n",
              " [106, 35, 36, 4, 31, 8, 107, 36, 108, 109, 110, 1],\n",
              " [106, 35, 36, 4, 31, 8, 107, 36, 108, 109, 110, 1, 111],\n",
              " [112, 6],\n",
              " [112, 6, 113],\n",
              " [112, 6, 113, 114],\n",
              " [112, 6, 113, 114, 3],\n",
              " [112, 6, 113, 114, 3, 115],\n",
              " [112, 6, 113, 114, 3, 115, 116],\n",
              " [112, 6, 113, 114, 3, 115, 116, 12],\n",
              " [112, 6, 113, 114, 3, 115, 116, 12, 117],\n",
              " [9, 10],\n",
              " [9, 10, 118],\n",
              " [9, 10, 118, 1],\n",
              " [9, 10, 118, 1, 119],\n",
              " [9, 10, 118, 1, 119, 18],\n",
              " [9, 10, 118, 1, 119, 18, 3],\n",
              " [9, 10, 118, 1, 119, 18, 3, 120],\n",
              " [9, 10, 118, 1, 119, 18, 3, 120, 121],\n",
              " [9, 10, 118, 1, 119, 18, 3, 120, 121, 122],\n",
              " [9, 10, 118, 1, 119, 18, 3, 120, 121, 122, 8],\n",
              " [9, 10, 118, 1, 119, 18, 3, 120, 121, 122, 8, 4],\n",
              " [123, 37],\n",
              " [123, 37, 2],\n",
              " [123, 37, 2, 5],\n",
              " [123, 37, 2, 5, 27],\n",
              " [123, 37, 2, 5, 27, 1],\n",
              " [123, 37, 2, 5, 27, 1, 124],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125, 6],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125, 6, 13],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125, 6, 13, 16],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125, 6, 13, 16, 17],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125, 6, 13, 16, 17, 3],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125, 6, 13, 16, 17, 3, 126],\n",
              " [123, 37, 2, 5, 27, 1, 124, 125, 6, 13, 16, 17, 3, 126, 1],\n",
              " [127, 11],\n",
              " [127, 11, 128],\n",
              " [127, 11, 128, 129],\n",
              " [127, 11, 128, 129, 130],\n",
              " [127, 11, 128, 129, 130, 131],\n",
              " [127, 11, 128, 129, 130, 131, 132],\n",
              " [127, 11, 128, 129, 130, 131, 132, 35],\n",
              " [127, 11, 128, 129, 130, 131, 132, 35, 133],\n",
              " [134, 135],\n",
              " [134, 135, 32],\n",
              " [134, 135, 32, 136],\n",
              " [134, 135, 32, 136, 4],\n",
              " [134, 135, 32, 136, 4, 33],\n",
              " [134, 135, 32, 136, 4, 33, 2],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138, 1],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138, 1, 7],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138, 1, 7, 34],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138, 1, 7, 34, 1],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138, 1, 7, 34, 1, 139],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138, 1, 7, 34, 1, 139, 140],\n",
              " [134, 135, 32, 136, 4, 33, 2, 137, 138, 1, 7, 34, 1, 139, 140, 4],\n",
              " [141, 28],\n",
              " [141, 28, 3],\n",
              " [141, 28, 3, 142],\n",
              " [141, 28, 3, 142, 143],\n",
              " [141, 28, 3, 142, 143, 8],\n",
              " [141, 28, 3, 142, 143, 8, 7],\n",
              " [141, 28, 3, 142, 143, 8, 7, 37],\n",
              " [141, 28, 3, 142, 143, 8, 7, 37, 23],\n",
              " [141, 28, 3, 142, 143, 8, 7, 37, 23, 9],\n",
              " [141, 28, 3, 142, 143, 8, 7, 37, 23, 9, 10],\n",
              " [19, 20]]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "CcQp7Jtw0Z8_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding=\"pre\")"
      ],
      "metadata": {
        "id": "P1HZkPDj1Xs_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUk5dq-d884R",
        "outputId": "9f686907-2abf-4099-cfe2-93af063392af"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0, 19, 20],\n",
              "       [ 0,  0,  0, ...,  0, 38, 39],\n",
              "       [ 0,  0,  0, ..., 38, 39, 40],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 37, 23,  9],\n",
              "       [ 0,  0,  0, ..., 23,  9, 10],\n",
              "       [ 0,  0,  0, ...,  0, 19, 20]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]\n",
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "-mFY-zDN1hV1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index)+1)"
      ],
      "metadata": {
        "id": "3i_SR1g-9LbF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-e6uR8N-rY1",
        "outputId": "a5e59507-b6c3-4fa0-be7f-0cee370b1f2b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the embedding layer with explicit input shape\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_len-1))\n",
        "\n",
        "# Add the LSTM layer\n",
        "model.add(LSTM(150))\n",
        "\n",
        "# Add the output layer\n",
        "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))\n",
        "\n",
        "# Build the model with input shape\n",
        "model.build(input_shape=(None, max_len-1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "67jzXTVj-sVi",
        "outputId": "a76a93bb-6858-4619-ee49-6c34c403a382"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │          \u001b[38;5;34m14,500\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │         \u001b[38;5;34m150,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m145\u001b[0m)                 │          \u001b[38;5;34m21,895\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">14,500</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">145</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">21,895</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m186,995\u001b[0m (730.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,995</span> (730.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m186,995\u001b[0m (730.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,995</span> (730.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " model.fit(X,y,epochs = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJy5z6kaAh82",
        "outputId": "db930124-7397-4a4e-cf6e-54d6e05a1a1c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.0224 - loss: 4.9767\n",
            "Epoch 2/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0368 - loss: 4.9523\n",
            "Epoch 3/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0673 - loss: 4.8675\n",
            "Epoch 4/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0570 - loss: 4.6909\n",
            "Epoch 5/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0448 - loss: 4.6211\n",
            "Epoch 6/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0529 - loss: 4.6225\n",
            "Epoch 7/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0507 - loss: 4.5732\n",
            "Epoch 8/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0696 - loss: 4.4727\n",
            "Epoch 9/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0567 - loss: 4.4189\n",
            "Epoch 10/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0683 - loss: 4.4112\n",
            "Epoch 11/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0522 - loss: 4.2919\n",
            "Epoch 12/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0638 - loss: 4.2601\n",
            "Epoch 13/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0662 - loss: 4.2045\n",
            "Epoch 14/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1012 - loss: 4.0774\n",
            "Epoch 15/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1028 - loss: 4.0357\n",
            "Epoch 16/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1032 - loss: 3.8746\n",
            "Epoch 17/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1127 - loss: 3.7712\n",
            "Epoch 18/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1589 - loss: 3.7298\n",
            "Epoch 19/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.1646 - loss: 3.5536\n",
            "Epoch 20/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2408 - loss: 3.4021\n",
            "Epoch 21/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2370 - loss: 3.2415\n",
            "Epoch 22/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2759 - loss: 3.1333\n",
            "Epoch 23/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3115 - loss: 3.0668\n",
            "Epoch 24/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.3186 - loss: 2.9573\n",
            "Epoch 25/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.3990 - loss: 2.8024\n",
            "Epoch 26/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3841 - loss: 2.7587\n",
            "Epoch 27/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.4393 - loss: 2.5688\n",
            "Epoch 28/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4664 - loss: 2.4364\n",
            "Epoch 29/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4526 - loss: 2.4018\n",
            "Epoch 30/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5789 - loss: 2.1756\n",
            "Epoch 31/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5108 - loss: 2.1840\n",
            "Epoch 32/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5812 - loss: 2.0664\n",
            "Epoch 33/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5752 - loss: 2.0403\n",
            "Epoch 34/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6090 - loss: 1.9090\n",
            "Epoch 35/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6388 - loss: 1.8712\n",
            "Epoch 36/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6778 - loss: 1.7661\n",
            "Epoch 37/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6935 - loss: 1.7428\n",
            "Epoch 38/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6921 - loss: 1.6634\n",
            "Epoch 39/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7479 - loss: 1.5456\n",
            "Epoch 40/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7038 - loss: 1.5550\n",
            "Epoch 41/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7280 - loss: 1.5107\n",
            "Epoch 42/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7734 - loss: 1.4347\n",
            "Epoch 43/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8041 - loss: 1.3441\n",
            "Epoch 44/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8150 - loss: 1.3153\n",
            "Epoch 45/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7947 - loss: 1.2877\n",
            "Epoch 46/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8794 - loss: 1.1752\n",
            "Epoch 47/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8636 - loss: 1.2116\n",
            "Epoch 48/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8921 - loss: 1.1157\n",
            "Epoch 49/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8828 - loss: 1.1207\n",
            "Epoch 50/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9123 - loss: 1.0295\n",
            "Epoch 51/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9273 - loss: 0.9825\n",
            "Epoch 52/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9483 - loss: 0.9484\n",
            "Epoch 53/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9678 - loss: 0.9014\n",
            "Epoch 54/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9614 - loss: 0.8400\n",
            "Epoch 55/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9682 - loss: 0.8403\n",
            "Epoch 56/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9743 - loss: 0.8099\n",
            "Epoch 57/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9763 - loss: 0.7877\n",
            "Epoch 58/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9909 - loss: 0.7433\n",
            "Epoch 59/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9890 - loss: 0.7217\n",
            "Epoch 60/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9758 - loss: 0.6475\n",
            "Epoch 61/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9910 - loss: 0.6624\n",
            "Epoch 62/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9799 - loss: 0.5887\n",
            "Epoch 63/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9870 - loss: 0.5994\n",
            "Epoch 64/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9919 - loss: 0.5754\n",
            "Epoch 65/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9877 - loss: 0.5631\n",
            "Epoch 66/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9919 - loss: 0.5559\n",
            "Epoch 67/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9905 - loss: 0.5096\n",
            "Epoch 68/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9800 - loss: 0.5095\n",
            "Epoch 69/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9946 - loss: 0.4355\n",
            "Epoch 70/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9938 - loss: 0.4845\n",
            "Epoch 71/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9874 - loss: 0.4395\n",
            "Epoch 72/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9925 - loss: 0.4427\n",
            "Epoch 73/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9952 - loss: 0.4271\n",
            "Epoch 74/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9974 - loss: 0.4081\n",
            "Epoch 75/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9974 - loss: 0.3565\n",
            "Epoch 76/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9988 - loss: 0.3619\n",
            "Epoch 77/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9951 - loss: 0.3468\n",
            "Epoch 78/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9974 - loss: 0.3307\n",
            "Epoch 79/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9974 - loss: 0.3109\n",
            "Epoch 80/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9982 - loss: 0.3257\n",
            "Epoch 81/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9925 - loss: 0.3038\n",
            "Epoch 82/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9931 - loss: 0.3062\n",
            "Epoch 83/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9988 - loss: 0.2818\n",
            "Epoch 84/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9931 - loss: 0.2790\n",
            "Epoch 85/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9964 - loss: 0.2573\n",
            "Epoch 86/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9982 - loss: 0.2472\n",
            "Epoch 87/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9982 - loss: 0.2538\n",
            "Epoch 88/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9951 - loss: 0.2352\n",
            "Epoch 89/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9951 - loss: 0.2263\n",
            "Epoch 90/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9952 - loss: 0.2219\n",
            "Epoch 91/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9982 - loss: 0.2133\n",
            "Epoch 92/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9892 - loss: 0.2185\n",
            "Epoch 93/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9892 - loss: 0.2139\n",
            "Epoch 94/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9951 - loss: 0.1973\n",
            "Epoch 95/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9951 - loss: 0.1855\n",
            "Epoch 96/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9892 - loss: 0.1991\n",
            "Epoch 97/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9913 - loss: 0.1842\n",
            "Epoch 98/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9931 - loss: 0.1826\n",
            "Epoch 99/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9952 - loss: 0.1695\n",
            "Epoch 100/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9951 - loss: 0.1681\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ff7f94708b0>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hiring for\"\n",
        "\n",
        "for i in range(5):\n",
        "  # tokenize\n",
        "  tokenized_text = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "  # pad\n",
        "  padded_text = pad_sequences([tokenized_text], maxlen=max_len-1, padding='pre')\n",
        "  # padding\n",
        "  # predic\n",
        "  pos = np.argmax(model.predict(padded_text))\n",
        "\n",
        "\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text += \" \" + word\n",
        "      print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Dmw4CaIgYG",
        "outputId": "88235b66-442d-466c-c733-104744356639"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Hiring for considering\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Hiring for considering my\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Hiring for considering my application\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Hiring for considering my application i\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Hiring for considering my application i look\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "text = \"Hiring\"\n",
        "tokenized_text = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# pad\n",
        "padded_text = pad_sequences([tokenized_text], maxlen=max_len-1, padding='pre')\n",
        "# padding\n",
        "# predic\n",
        "pos = np.argmax(model.predict(padded_text))\n",
        "\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index == pos:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwBIwBWSIt8g",
        "outputId": "1616a522-f0d9-474f-8d10-388fae2e18d9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "manager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4IAGEgjJiPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}